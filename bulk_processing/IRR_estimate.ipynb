{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Applies a table filter across all extracted tables from a project and calculates \n",
    "# the net underwriter discount, and\n",
    "# the face value\n",
    "# for a specific type of table (standard case: two column with $ denominated key-value pairs)\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "import string\n",
    "\n",
    "sys.path.insert(0, os.path.pardir)\n",
    "\n",
    "from backend import *\n",
    "from data_query import *\n",
    "\n",
    "UPLOAD_FOLDER = os.path.join('..', 'static', 'ug')\n",
    "FILTER_FOLDER = os.path.join('..', 'static', 'filters')\n",
    "PROJECT = 'muni_bonds_bulk_2'\n",
    "FILTER = 'funds'\n",
    "\n",
    "path = os.path.join(UPLOAD_FOLDER, PROJECT, '*.tables.json')\n",
    "table_files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with filter {u'headers': {u'threshold': 0.35, u'terms': [u'USES OF FUNDS']}, u'name': u'Estimated use and sources of funds'}\n",
      "Procssing with value dictionary {'underwriter_discount': 'Underwriter Discount', 'premium': 'Issue Premium', 'premium_or_discount': 'Premium Discount', 'discount': 'Issue Discount', 'face_value': ['Principal Amount', 'Par Amount', 'Face Amount'], 'cost_of_issuance': 'Costs of Issuance'}\n"
     ]
    }
   ],
   "source": [
    "def clean_string(s):\n",
    "    lc = s.encode('ascii', errors='ignore').lower()#.translate(remove_punctuation_map)\n",
    "    return lc.translate(None, string.punctuation + '0123456789').strip()\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "table_counter = Counter()\n",
    "tables_looked_at = 0\n",
    "confidences = []\n",
    "no_table_files = []\n",
    "no_ud_tables = []\n",
    "no_fv_tables = []\n",
    "funny_tables = {}\n",
    "\n",
    "salient_values = {}\n",
    "\n",
    "# Get those line items sufficient for IRR estimation\n",
    "# remark: improved query terms from TF analysis and annotation\n",
    "irr_estimate_dict = {'face_value' : ['Principal Amount', 'Par Amount', 'Face Amount'], \n",
    "                     'premium' : 'Issue Premium',\n",
    "                     'discount': 'Issue Discount',\n",
    "                     'premium_or_discount' : 'Premium Discount', #will match line items that signify either at high confidence on the token level\n",
    "                     'underwriter_discount' : 'Underwriter Discount',\n",
    "                     'cost_of_issuance' : 'Costs of Issuance'}\n",
    "\n",
    "filter_file = os.path.join(FILTER_FOLDER, FILTER+'.json')\n",
    "with codecs.open(filter_file, \"r\", \"utf-8\", errors=\"replace\") as file:\n",
    "    _filter = json.load(file) \n",
    "\n",
    "print (\"Processing with filter %s\" % str(_filter))\n",
    "print (\"Procssing with value dictionary %s\" % str(irr_estimate_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get all tables\n",
    "for i,f in enumerate(table_files):\n",
    "\n",
    "    with codecs.open(f, 'r', 'utf-8') as file:\n",
    "        tables = json.load(file)\n",
    "        tables_looked_at += len(tables)\n",
    "        \n",
    "        filename = f.split(r'/')[-1].replace('.tables.json', '')\n",
    "        \n",
    "        filter_results = []\n",
    "        for t in filter_tables(tables.values(), _filter):\n",
    "            if len(filter_results) == 0 or t[0] >= max(r[0] for r in filter_results):\n",
    "                filter_results.append(t)\n",
    "        \n",
    "        table_counter[len(filter_results)] += 1        \n",
    "        if len(filter_results):\n",
    "\n",
    "            #Only keep first one\n",
    "            confidence, table, _, _ = max( sorted( filter_results, key = lambda t: t[1]['begin_line'] ), \n",
    "                                          key = lambda t: t[0])\n",
    "            confidences.append(confidence)\n",
    "            if len(table['captions']) != 2 or table['subtypes'][1] != 'dollar':\n",
    "                funny_tables[filename] = table['begin_line']\n",
    "            \n",
    "            else:\n",
    "                values = get_key_values(table, irr_estimate_dict, raw_cell=True)\n",
    "                #invert line item if in brackets\n",
    "                if values['premium_or_discount']:\n",
    "                    r = values['premium_or_discount'][1]\n",
    "                    if 'leftover' in r and '(' in r['leftover'][0] and ')' in r['leftover'][1]:\n",
    "                        values['premium_or_discount'][0] = values['premium_or_discount'][0]\n",
    "                \n",
    "                #strip raw rows\n",
    "                values = {k : (v[0] if v else None) for k,v in values.iteritems()}\n",
    "                key = filename+'#'+str(table['begin_line'])\n",
    "                \n",
    "                if not values['face_value']: \n",
    "                    no_fv_tables.append(key)\n",
    "\n",
    "                if not values['underwriter_discount']: \n",
    "                    no_ud_tables.append(key)\n",
    "                    \n",
    "                #maybe problem with ordering guarantee\n",
    "                salient_values[key] = values.values()\n",
    "\n",
    "        else:\n",
    "            no_table_files.append(filename)\n",
    "        \n",
    "    if ( (i+1) % 100 ) == 0:\n",
    "        print (\"%i files and %i tables processed... with %i best matches\" % \\\n",
    "               (i+1, tables_looked_at, len(confidences)))\n",
    "\n",
    "        \n",
    "results = {'high_confidence_candidates' : table_counter.most_common(),\n",
    "           'tables_looked_at' : tables_looked_at,\n",
    "           'tables_canonical' : len(confidences),\n",
    "           'confidence_mean' : sum(confidences) / len(confidences),\n",
    "           'confidences' : confidences, \n",
    "           'no_table_files' : no_table_files,\n",
    "           'no_ud_tables' : no_ud_tables,\n",
    "           'no_fv_tables' : no_fv_tables,\n",
    "           'funny_tables' : funny_tables,\n",
    "           'salient_values' : salient_values\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save intermediate results\n",
    "with codecs.open(\"IRR_estimate.results.json\", \"w\", \"utf-8\") as file:\n",
    "    json.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Work from intermediate results\n",
    "with codecs.open(\"IRR_estimate.results.json\", \"r\", \"utf-8\") as file:\n",
    "    results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "\n",
    "bold = xlwt.Style.easyxf(\"font: bold on\")\n",
    "\n",
    "def write_table(sheet, keys, values, row, c_offset = 0, column_style = bold):\n",
    "    for j, k in enumerate(keys):\n",
    "        sheet.write(row, c_offset+j, k, column_style)\n",
    "    row += 1\n",
    "    for v in values:\n",
    "        for j, vv in enumerate(v):\n",
    "            sheet.write(row, c_offset+j, vv)\n",
    "        row +=1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_prefix = \"http://tabularazr.eastus.cloudapp.azure.com:7081/show/\"+PROJECT+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_xls_url(url, link = None):\n",
    "    f = 'HYPERLINK(\"'+url+'\"' + ('; \"'+link+'\")' if link else ')')\n",
    "    return xlwt.Formula(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wkb = xlwt.Workbook(encoding='utf-8')\n",
    "s_summary, s_funding_values, s_confidence, s_no_table, s_no_fv_tables, s_no_ud_tables, s_funny_tables = \\\n",
    "    (wkb.add_sheet(s) for s in ['summary', 'funding_values', 'confidence', 'no_table', \n",
    "                                'no_face_value_tables', 'no_underwriter_discount_tables', 'funny_tables'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "s_summary.write(i,0, 'Filter used', bold)\n",
    "s_summary.write(i,1, str(_filter))\n",
    "i+=1\n",
    "s_summary.write(i,0, 'Value extraction dictionary used', bold)\n",
    "s_summary.write(i,1, str(irr_estimate_dict))\n",
    "i+=2\n",
    "s_summary.write(i,0, 'Distribution of good table matches per document', bold)\n",
    "i+=1\n",
    "i = write_table(s_summary, ['Nr. of Table Candidates', 'Nr. of Documents'], \n",
    "                results[\"high_confidence_candidates\"], i)\n",
    "\n",
    "i+=1\n",
    "s_summary.write(i, 2, 'Total nr. of Table Candidates')\n",
    "s_summary.write(i, 3, 'out of..')\n",
    "i+=1\n",
    "s_summary.write(i, 2, results['tables_canonical'])\n",
    "s_summary.write(i, 3, results['tables_looked_at'])\n",
    "\n",
    "i = write_table(s_confidence, ['Confidence in best Table found'], ([c] for c in results['confidences']), 0)\n",
    "i = write_table(s_no_table, ['Files with no suitable table found', 'URL'], \n",
    "                ( ([c], to_xls_url(url_prefix+c)) for c in results['no_table_files'] ), 0)\n",
    "i = write_table(s_no_ud_tables, ['Tables with no Underwriter Discount found', 'URL'], \n",
    "                ( ([c], to_xls_url(url_prefix+c)) for c in results['no_ud_tables'] ), 0)\n",
    "i = write_table(s_no_fv_tables, ['Tables with no Face Value found', 'URL'], \n",
    "                ( ([c], to_xls_url(url_prefix+c)) for c in results['no_fv_tables'] ), 0)\n",
    "\n",
    "\n",
    "s_funny_tables.write(0,4, \"[as returned by filter but with <> 2 rows, and/or no $ value in the 2nd column]\")\n",
    "i = write_table(s_funny_tables, ['Funny Tables in File', 'Table ID',  'URL'], \n",
    "                ( ( f, t, to_xls_url(url_prefix+f+'#'+str(t)) ) for f, t in results['funny_tables'].iteritems() ), 0)\n",
    "\n",
    "header_funding_values = ['Filename/Table', 'URL'] + irr_estimate_dict.keys()\n",
    "i = write_table(s_funding_values, header_funding_values, \n",
    "               (( [k, to_xls_url(url_prefix+k)] + v) for k, v in results['salient_values'].iteritems()), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wkb.save('IRR_estimate.results.xls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
